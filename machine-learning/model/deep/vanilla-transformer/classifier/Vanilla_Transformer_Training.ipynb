{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zouAlIITIJY",
        "outputId": "bfccdb42-6b9b-4b05-a096-0aa2fff43df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<0.17,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub, tokenizers\n",
            "Successfully installed huggingface_hub-0.16.4 tokenizers-0.14.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tokenizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential,Model\n",
        "from tokenizers import Tokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionalEmbedding, self).__init__()\n",
        "        self.token_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.positional_embedding = layers.Embedding(input_dim=seq_len, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, input):\n",
        "        len = tf.shape(input)[-1]\n",
        "        positions = tf.range(start=0, limit=len, delta=1)\n",
        "        positions = self.positional_embedding(positions)\n",
        "        tokens = self.token_embedding(input)\n",
        "        return tokens + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"token_embedding\": self.token_embedding, \"positional_embedding\": self.positional_embedding}"
      ],
      "metadata": {
        "id": "Bd4vkapZULe2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.attn_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.l_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.l_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.attn_layer(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.l_norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(out1, training=training)\n",
        "        return self.l_norm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"attn_layer\": self.attn_layer, \"ffn\": self.ffn, \"l_norm1\": self.l_norm1, \"l_norm2\": self.l_norm2, \"dropout1\": self.dropout1, \"dropout2\": self.dropout2}"
      ],
      "metadata": {
        "id": "ZW9Ly-6iTQz3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehotencoder_path = \"/content/drive/MyDrive/SAIL Exam Datasets/label-encoder.pickle\"\n",
        "with open(onehotencoder_path, 'rb') as handle:\n",
        "    label_encoder = pickle.load(handle)"
      ],
      "metadata": {
        "id": "mfY38Z8BUV4d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_path = \"/content/drive/MyDrive/SAIL Exam Datasets/tweets_reviews/bpe-tokenizer-tweets-reviews.json\"\n",
        "tokenizer = Tokenizer.from_file(tokenizer_path)"
      ],
      "metadata": {
        "id": "L8SF-d4RVO3s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\"DROPOUT\": 0.1, \"LABEL_COUNT\": 3, \"LEARNING_RATE\": 0.00005, \"BATCH_SIZE\":64,\n",
        "                   \"VOCAB_SIZE\": tokenizer.get_vocab_size(), \"SEQ_LEN\": 10000, \"EMBED_DIM\": 1, \"NUM_HEADS\": 1, \"FF_DIM\": 1\n",
        "                  }"
      ],
      "metadata": {
        "id": "fbmCZFjnVVP4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.layers.Input(shape=(hyperparameters[\"SEQ_LEN\"],))\n",
        "embedding_layer = TokenAndPositionalEmbedding(hyperparameters[\"SEQ_LEN\"], tokenizer.get_vocab_size(), hyperparameters[\"EMBED_DIM\"])\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(hyperparameters[\"EMBED_DIM\"], hyperparameters[\"NUM_HEADS\"], hyperparameters[\"FF_DIM\"])\n",
        "x = transformer_block(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf.keras.layers.Dropout(hyperparameters[\"DROPOUT\"])(x)\n",
        "x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(hyperparameters[\"DROPOUT\"])(x)\n",
        "x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(hyperparameters[\"LABEL_COUNT\"], activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "Uz6i1c8WVcpH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"LEARNING_RATE\"], amsgrad=True)"
      ],
      "metadata": {
        "id": "bree_pIpVjAh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "qzLJo3c0VlHM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path,batch_size):\n",
        "    df = pd.read_csv(path, sep=';')\n",
        "    df = df[df.text.notna()]\n",
        "    tokenized = df.text.apply(lambda row: tokenizer.encode(row).ids).tolist()\n",
        "    tokenized_tf = tf.convert_to_tensor(tokenized)\n",
        "    target = label_encoder.transform(np.array(df.expected_sentiment).reshape(-1,1)).toarray()\n",
        "    target_tf = tf.convert_to_tensor(target)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((tokenized_tf, target_tf))\n",
        "    dataset = dataset.shuffle(100).batch(batch_size)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "_w3m4FZKVmbl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(\"/content/drive/MyDrive/SAIL Exam Datasets/tweets/train.csv\", 128)\n",
        "val_dataset = create_dataset(\"/content/drive/MyDrive/SAIL Exam Datasets/tweets/val.csv\", 128)"
      ],
      "metadata": {
        "id": "ISAzZCvCVo9s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "KYTLG1Uicq01",
        "outputId": "18548a60-2338-4a15-948f-c0198df73a25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-48ea4120bdc1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QhGZ-Vfc_3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}